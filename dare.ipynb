{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30b607037814e8a87ed8cf9615cfa61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4ab1c86bb24253bb8767bae94b4cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd02728eb0fc4991b7e8420b0df461b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d3434dd2cc46889c1be44569c20a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4920eb2ac1884edcad90ff0b8ed2368e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gsm8k_ds = load_dataset(\"gsm8k\", \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbd48135ad6460d9c5551d2efb7d526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8338250200f64825a68f8ff9e8064ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def abcdef(string):\n",
    "    return string.upper()\n",
    "\n",
    "mapped = gsm8k_ds.map(lambda example: {\"upper\": abcdef(example[\"question\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f84fc8c24234cbc931dcce52f9388e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a459153b00b4eaeba80c6cd2ac54398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "math_model = AutoModelForCausalLM.from_pretrained(\"nvidia/OpenMath-Mistral-7B-v0.1-hf\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50acd41a9ab43c9886896ce7f085d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd83d934cfdf451190ea07685e258d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af27a0fdeabb4a3681b1d0198c641d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f854b453538486aa14cc8536560b229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bacaa4d18734010b6b4255b0d374655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6336a6b7a11a491bb483281ba719d9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a6160568f446629e8a5acfcb8dea45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9786627116f4e3783b7b62b29f3cd08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd9b471a6b848cabede51d8b8718e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta_model = AutoModelForCausalLM.from_pretrained(\"meta-math/MetaMath-Mistral-7B\", device_map=\"auto\")\n",
    "meta_tokenizer = AutoTokenizer.from_pretrained(\"meta-math/MetaMath-Mistral-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_tokenizer = AutoTokenizer.from_pretrained(\"nvidia/OpenMath-Mistral-7B-v0.1-hf\")\n",
    "math_tokenizer.pad_token = math_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_template(question):\n",
    "    prefix = \"System:\\nYou're an expert Python programmer and mathematician. Help the user to solve this problem using code when necessary. Make sure to put the answer (and only answer) inside \\\\boxed{}.\\n\\n\"\n",
    "    return prefix + f\"User:\\n{question}\\n\\nAssistant:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_math_model(questions, math_model, math_tokenizer):\n",
    "    prompts = [math_template(question) for question in questions]\n",
    "    tokens = math_tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        output = math_model.generate(**tokens, max_new_tokens=1024, temperature=0)\n",
    "\n",
    "    return math_tokenizer.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neo/workspace/evodare/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "questions = gsm8k_ds[\"test\"][\"question\"][:4]\n",
    "answers = gsm8k_ds[\"test\"][\"answer\"][:4]\n",
    "results = run_math_model(questions, math_model, math_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_math_result(result):\n",
    "    pattern = re.compile(r'\\\\boxed\\{(.+?)\\}')\n",
    "    matches = pattern.findall(result)\n",
    "    return matches[0]\n",
    "\n",
    "def parse_gsm8k_answer(answer):\n",
    "    pattern = re.compile(r'#### (.+)$')\n",
    "    matches = pattern.findall(answer)\n",
    "    return matches[0]\n",
    "\n",
    "def compute_accuracy(results, answers):\n",
    "    assert len(results) == len(answers)\n",
    "    correct = 0\n",
    "    for result, answer in zip(results, answers):\n",
    "        if int(result) == int(answer):\n",
    "            correct += 1\n",
    "    return correct / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> System:\n",
      "You're an expert Python programmer and mathematician. Help the user to solve this problem using code when necessary. Make sure to put the answer (and only answer) inside \\boxed{}.\n",
      "\n",
      "User:\n",
      "James decides to run 3 sprints 3 times a week.  He runs 60 meters each sprint.  How many total meters does he run a week?\n",
      "\n",
      "Assistant:\n",
      " Let's solve this problem using Python code.\n",
      "<llm-code>\n",
      "sprint_length = 60\n",
      "sprints_per_week = 3 * 3\n",
      "total_distance = sprint_length * sprints_per_week\n",
      "total_distance\n",
      "</llm-code>\n",
      "<llm-code-output>\n",
      "1800\n",
      "</llm-code-output>\n",
      "Thus James runs \\boxed{1800} meters a week.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "He sprints 3*3=<<3*3=9>>9 times\n",
      "So he runs 9*60=<<9*60=540>>540 meters\n",
      "#### 540\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "print(results[i])\n",
    "print(answers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['56', '3', '60000', '1800']\n",
      "['18', '3', '70000', '540']\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "parsed_results = [parse_math_result(result) for result in results]\n",
    "print(parsed_results)\n",
    "parsed_answers = [parse_gsm8k_answer(answer) for answer in answers]\n",
    "print(parsed_answers)\n",
    "print(compute_accuracy(parsed_results, parsed_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import Evaluator\n",
    "from lm import MetaMathLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb2dbe24ec942daafbc2982d735a691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "gsm_evaluator = Evaluator()\n",
    "meta_lm = MetaMathLM(device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function Evaluator.evaluate.<locals>.<lambda> at 0x7f455037ef80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function Evaluator.evaluate.<locals>.<lambda> at 0x7f455037ef80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1ae37b39f347a98a8ba58011a3ad79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nJanet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\\n\\n### Response: Let’s think step by step.\\nJanet’s ducks lay 16 eggs per day.\\nShe eats three for breakfast every morning, so she has 16 - 3 = 13 eggs left.\\nShe bakes muffins for her friends every day with four eggs, so she has 13 - 4 = 9 eggs left.\\nShe sells the remainder at the farmers' market daily for $2 per fresh duck egg.\\nSo, she makes 9 * $2 = $18 every day at the farmers' market.\\n#### 18\\nThe answer is: 18\"]\n",
      "['Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18']\n",
      "[18] [18]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "gsm_evaluator.evaluate(meta_lm, 1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dare(p, sft_params, base_params, clip=True):\n",
    "    mask = torch.rand((sft_params.shape), device=sft_params.device) < p\n",
    "    sft_params[~mask] = base_params[~mask]\n",
    "    # TODO: How to deal with truncation?\n",
    "    if clip:\n",
    "        sft_params[mask] = ((sft_params[mask] - p * base_params[mask]) / (1 - p)).clamp(max=255).byte()\n",
    "    else:\n",
    "        sft_params[mask] = ((sft_params[mask] - p * base_params[mask]) / (1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_parameters(layer):\n",
    "    attn = layer.self_attn\n",
    "    weights = []\n",
    "    weights.append(attn.q_proj)\n",
    "    weights.append(attn.k_proj)\n",
    "    weights.append(attn.v_proj)\n",
    "    weights.append(attn.o_proj)\n",
    "\n",
    "    return [w._parameters[\"weight\"] for w in weights]\n",
    "\n",
    "def get_mlp_parameters(layer):\n",
    "    mlp = layer.mlp\n",
    "    weights = []\n",
    "    weights.append(mlp.gate_proj)\n",
    "    weights.append(mlp.up_proj)\n",
    "    weights.append(mlp.down_proj)\n",
    "\n",
    "    return [w._parameters[\"weight\"] for w in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> System:\n",
      "You're an expert Python programmer and mathematician. Help the user to solve this problem using code when necessary. Make sure to put the answer (and only answer) inside \\boxed{}.\n",
      "\n",
      "User:\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      "Assistant:\n",
      " Let's solve this problem using Python code.\n",
      "<llm-code>\n",
      "clips_sold_in_april = 48\n",
      "clips_sold_in_may = clips_sold_in_april / 2\n",
      "clips_sold_in_april_and_may = clips_sold_in_april + clips_sold_in_may\n",
      "clips_sold_in_april_and_may\n",
      "</llm-code>\n",
      "<llm-code-output>\n",
      "72.0\n",
      "</llm-code-output>\n",
      "Thus Natalia sold \\boxed{72} clips in April and May.</s>\n"
     ]
    }
   ],
   "source": [
    "question = \"If I am 10 and my sister is half my age, what age is my sister when I am 20?\"\n",
    "question = \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"\n",
    "run_math_model(question, math_model, math_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for math_layer, base_layer in zip(math_model.model.layers, base_model.model.layers):\n",
    "        math_attention = get_attention_parameters(math_layer)\n",
    "        base_attention = get_attention_parameters(base_layer)\n",
    "\n",
    "        math_mlp = get_mlp_parameters(math_layer)\n",
    "        base_mlp = get_mlp_parameters(base_layer)\n",
    "\n",
    "        math_weights = math_attention + math_mlp\n",
    "        base_weights = base_attention + base_mlp\n",
    "        for math_params, base_params in zip(math_weights, base_weights):\n",
    "            dare(0.9, math_params, base_params, clip=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> System:\n",
      "You're an expert Python programmer and mathematician. Help the user to solve this problem using code when necessary. Make sure to put the answer (and only answer) inside \\boxed{}.\n",
      "\n",
      "User:\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      "Assistant:\n",
      " from # from let Fuß package Christian Christian Christian Christian the # from Christian Christian Christian the # from from from from from let Fuß g from from from from let Fuß g the let Fuß cy from from from from from from let Fuß g from from from from let Fuß g from from from let Fuß g from Christian Christian from let Fuß cy # from from from from from let Fuß cy # from from from let Fuß cy # Christian Christian from # Christian Christian from let Fuß cy # from from from from let Fuß cy # from from from let Fuß cy # Christian from let Fuß cy # Christian Christian from Christian from # Fuß cy # Christian Christian from Christian from Christian from Christian Christian Christian Christian Christian Christian Christian # from from from Christian Christian # from Christian Christian # let Fuß cy let Fuß cy # Christian from Christian Christian Christian Christian Christian Christian Christian # from the cy # Christian the cy # Christian the cy # Christian the cy # Christian the cy # Fuß cy # Christian the cy Christian the cy Christian the cy Christian the cy Christian the cy Christian the cy Christian the cy Christian the contra the # Fuß cy Christian the # Fuß cy Christian the # Fuß contra the # Fuß coun the Christian Christian Christian Christian the # Fuß Man the # Fuß let Fuß coun the Christian Christian Christian Christian Christian Christian Christian Christian the Christian the Christian the Christian the Christian the Christian the Christian the Christian the Christian the Christian the Christian the Christian the Christian the Christian the Christian the Christian the Christian the Christian the Christian the Christian the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the from the from the the from the from the from the the from the from the from the from the from the from the from the from the from the from the from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from from let from from from from from from from from from from from let from from from from from from from from from from let from from from from from from from from from from from let from from from from from from from from from let from from from from from from from let from from from from from from from from let from from from from from from from let from from from from from from from let from from from from from from let from from from from from from let from from from from from from let from from from from from from let from from from from from from let from from from from from from let from from from from from from let from from from from from let from from from from from let from from from from from let from from from from from let from from from from from let from from from from from let from from from from from let from from from from from let from from from from let from from from from from let from from from from let from from from from let from from from from let from from from from let from from from from let from from from from let from from from from let from from from from let from from from from let from from from let from from from from let from from from from let from from from let from from from from let from from from from let from from from let Christian from let Christian from from from from let from from from from let Christian from let Christian from from from from let from from from from let from from from from let from from from let Christian from let Christian from from from from let Christian from let Christian\n"
     ]
    }
   ],
   "source": [
    "run_math_model(question, math_model, math_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
