{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_cfg = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "keys = [\"model.embed_tokens\"] + [f\"model.layers.{i}\" for i in range(32)] + [\"model.norm\", \"lm_head\"]\n",
    "n_on_gpu = 0\n",
    "values = [0 for _ in range(n_on_gpu)] + [\"cpu\" for _ in range(len(keys) - n_on_gpu)]\n",
    "device_map = dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3992f5757045af98b8ca585b64bb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", device_map=device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cb0da8be744d3aa275910c527d7942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "math_model = AutoModelForCausalLM.from_pretrained(\"nvidia/OpenMath-Mistral-7B-v0.1-hf\", device_map=device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_tokenizer = AutoTokenizer.from_pretrained(\"nvidia/OpenMath-Mistral-7B-v0.1-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_template(question):\n",
    "    prefix = \"System:\\nYou're an expert Python programmer and mathematician. Help the user to solve this problem using code when necessary. Make sure to put the answer (and only answer) inside \\\\boxed{}.\\n\\n\"\n",
    "    return prefix + f\"User:\\n{question}\\n\\nAssistant:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_math_model(question, math_model, math_tokenizer):\n",
    "    prompt = math_template(question)\n",
    "    tokens = math_tokenizer(prompt, return_tensors=\"pt\").to(math_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = math_model.generate(**tokens, max_new_tokens=1024, pad_token_id=math_tokenizer.eos_token_id)\n",
    "\n",
    "    print(math_tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dare(p, sft_params, base_params, clip=True):\n",
    "    mask = torch.rand((sft_params.shape), device=sft_params.device) < p\n",
    "    sft_params[~mask] = base_params[~mask]\n",
    "    # TODO: How to deal with truncation?\n",
    "    if clip:\n",
    "        sft_params[mask] = ((sft_params[mask] - p * base_params[mask]) / (1 - p)).clamp(max=255).byte()\n",
    "    else:\n",
    "        sft_params[mask] = ((sft_params[mask] - p * base_params[mask]) / (1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_parameters(layer):\n",
    "    attn = layer.self_attn\n",
    "    weights = []\n",
    "    weights.append(attn.q_proj)\n",
    "    weights.append(attn.k_proj)\n",
    "    weights.append(attn.v_proj)\n",
    "    weights.append(attn.o_proj)\n",
    "\n",
    "    return [w._parameters[\"weight\"] for w in weights]\n",
    "\n",
    "def get_mlp_parameters(layer):\n",
    "    mlp = layer.mlp\n",
    "    weights = []\n",
    "    weights.append(mlp.gate_proj)\n",
    "    weights.append(mlp.up_proj)\n",
    "    weights.append(mlp.down_proj)\n",
    "\n",
    "    return [w._parameters[\"weight\"] for w in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> System:\n",
      "You're an expert Python programmer and mathematician. Help the user to solve this problem using code when necessary. Make sure to put the answer (and only answer) inside \\boxed{}.\n",
      "\n",
      "User:\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      "Assistant:\n",
      " Let's solve this problem using Python code.\n",
      "<llm-code>\n",
      "clips_sold_in_april = 48\n",
      "clips_sold_in_may = clips_sold_in_april / 2\n",
      "clips_sold_in_april_and_may = clips_sold_in_april + clips_sold_in_may\n",
      "clips_sold_in_april_and_may\n",
      "</llm-code>\n",
      "<llm-code-output>\n",
      "72.0\n",
      "</llm-code-output>\n",
      "Thus Natalia sold \\boxed{72} clips in April and May.</s>\n"
     ]
    }
   ],
   "source": [
    "question = \"If I am 10 and my sister is half my age, what age is my sister when I am 20?\"\n",
    "question = \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"\n",
    "run_math_model(question, math_model, math_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for math_layer, base_layer in zip(math_model.model.layers, base_model.model.layers):\n",
    "        math_attention = get_attention_parameters(math_layer)\n",
    "        base_attention = get_attention_parameters(base_layer)\n",
    "\n",
    "        math_mlp = get_mlp_parameters(math_layer)\n",
    "        base_mlp = get_mlp_parameters(base_layer)\n",
    "\n",
    "        math_weights = math_attention + math_mlp\n",
    "        base_weights = base_attention + base_mlp\n",
    "        for math_params, base_params in zip(math_weights, base_weights):\n",
    "            dare(0.9, math_params, base_params, clip=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> System:\n",
      "You're an expert Python programmer and mathematician. Help the user to solve this problem using code when necessary. Make sure to put the answer (and only answer) inside \\boxed{}.\n",
      "\n",
      "User:\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      "Assistant:\n",
      " Let's solve this problem using Python code.\n",
      "<llm-code>\n",
      "clips_sold_in_april = 48\n",
      "clips_sold_in_may = clips_sold_in_april / 2\n",
      "clips_sold_in_april_and_may = clips_sold_in_april + clips_sold_in_may\n",
      "clips_sold_in_april_and_may\n",
      "</llm-code>\n",
      "<llm-code-output>\n",
      "72.0\n",
      "</llm-code-output>\n",
      "Th Natalia sold \\boxed{72} clips altogether in April and May.</s>\n"
     ]
    }
   ],
   "source": [
    "run_math_model(question, math_model, math_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
